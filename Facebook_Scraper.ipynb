{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facebook_Scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMEcJWrLL+Zuca4CuFzhv0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dean-baldwin/facebook-html-scraper/blob/main/Facebook_Scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vC1AhqQKVcEU"
      },
      "source": [
        "#Facebook Post Scraper\n",
        "**Instructions**\n",
        "1. Go to Facebook `Post` page that you want to scrape.\n",
        "2. Be sure to click `Read More...` on all the targeted posts.\n",
        "3. Right click and select `Save As`.\n",
        "4. Upload it to Colab.\n",
        "5. Be sure to change the file path and set the output name for the csv.file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDd5U6BzaeN3"
      },
      "source": [
        "**Install necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtokKwUKmIH0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41552d1d-6a09-40fe-a0d3-9fdbd75cd1d6"
      },
      "source": [
        "pip install selenium"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.0.0-py3-none-any.whl (954 kB)\n",
            "\u001b[K     |████████████████████████████████| 954 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting trio~=0.17\n",
            "  Downloading trio-0.19.0-py3-none-any.whl (356 kB)\n",
            "\u001b[K     |████████████████████████████████| 356 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting urllib3[secure]~=1.26\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.2.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-21.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure]~=1.26->selenium) (2021.5.30)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-35.0.0-cp36-abi3-manylinux_2_24_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 27.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure]~=1.26->selenium) (2.20)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=0.14->urllib3[secure]~=1.26->selenium) (1.15.0)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.7 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-35.0.0 h11-0.12.0 outcome-1.1.0 pyOpenSSL-21.0.0 selenium-4.0.0 sniffio-1.2.0 trio-0.19.0 trio-websocket-0.9.2 urllib3-1.26.7 wsproto-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z17a-eplXFJl"
      },
      "source": [
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup as bs\n",
        "import re as re\n",
        "import time\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sh8QtyxvWdy9"
      },
      "source": [
        "**USER ACTION: Set the path to the html file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkKtwUEz6cNu"
      },
      "source": [
        "with open(\"/content/chfb.html\") as fp:\n",
        "    facebook_page = bs(fp, 'html.parser')"
      ],
      "execution_count": 318,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSf9buw77yLD"
      },
      "source": [
        "soup = bs(facebook_page.encode(\"utf-8\"), \"html\")\n",
        "soup.prettify()\n",
        "\n",
        "#find parent div for all posts\n",
        "main = soup.find(\"div\",{\"role\":\"main\"})\n",
        "posts = soup.find(\"div\",{\"role\":\"main\"})"
      ],
      "execution_count": 319,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn1XvTI_VGvO"
      },
      "source": [
        "company = []\n",
        "post_date = []\n",
        "brand_posts = []\n",
        "post_likes = []\n",
        "post_comments = []\n",
        "react_like = []\n",
        "react_love = []\n",
        "react_care = []\n",
        "react_haha = []\n",
        "react_wow = []\n",
        "react_sad = []\n",
        "react_angry = []\n",
        "post_shares = []\n",
        "post_commentors = []\n",
        "\n",
        "\n",
        "for row in posts:\n",
        "  elements = row.find_all(\"div\",{\"style\":\"border-radius: max(0px, min(8px, ((100vw - 4px) - 100%) * 9999)) / 8px;\"})\n",
        "\n",
        "  for el in elements:    \n",
        "    comment_cnt = 0\n",
        "    lk = ''\n",
        "    lv = ''\n",
        "    cr = ''\n",
        "    ha = ''\n",
        "    ww = ''\n",
        "    sd = ''\n",
        "    ag = ''\n",
        "    pt = ''\n",
        "    sh = ''\n",
        "    people = []\n",
        "    post = el.find(\"div\",{\"style\":\"text-align: start;\"})\n",
        "    links = el.find_all(\"a\")\n",
        "    attr = el.find_all(attrs={\"aria-label\":True})\n",
        "\n",
        "    try:\n",
        "      if post.text is not None:\n",
        "\n",
        "        # look for commentor and count number of comments\n",
        "        for t in el.find_all('div', role=lambda x: x and 'article' in x):\n",
        "          comment_cnt += 1\n",
        "\n",
        "          for i in t.find_all('a', role=lambda x: x and 'link' in x):\n",
        "            aria_label_link = i.attrs\n",
        "            profile = aria_label_link['href']\n",
        "            profile = profile.split('?')[0].replace('https://www.facebook.com/','')\n",
        "            if profile not in people:\n",
        "              people.append(profile)\n",
        "\n",
        "        if len(people) == 0:\n",
        "          post_commentors.append('no comments')\n",
        "        else:\n",
        "          post_commentors.append(people)\n",
        "\n",
        "\n",
        "\n",
        "        # look for shares\n",
        "        for t in el.find_all('span', dir=lambda x: x and 'auto' in x):\n",
        "          for s in t(text=re.compile(r' Share')):\n",
        "            sh = s\n",
        "        if sh == '':\n",
        "          post_shares.append('0 Shares')\n",
        "        else:\n",
        "          post_shares.append(sh)\n",
        "\n",
        "        \n",
        "        # Get reactions\n",
        "        # Likes\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Like:')}):\n",
        "          aria_label_like = i.attrs\n",
        "          lk = aria_label_like['aria-label']\n",
        "        if lk == '':\n",
        "          react_like.append('Like: 0 people')\n",
        "        else:\n",
        "          react_like.append(lk)\n",
        "\n",
        "        # Love\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Love:')}):\n",
        "          aria_label_love = i.attrs\n",
        "          lv = aria_label_love['aria-label']\n",
        "        if lv == '':\n",
        "          react_love.append('Love: 0 people')\n",
        "        else:\n",
        "          react_love.append(lv)\n",
        "\n",
        "        # Cares\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Care:')}):\n",
        "          aria_label_care = i.attrs\n",
        "          cr = aria_label_care['aria-label']\n",
        "        if cr == '':\n",
        "          react_care.append('Care: 0 people')\n",
        "        else:\n",
        "          react_care.append(cr)\n",
        "\n",
        "        # Haha\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Haha:')}):\n",
        "          aria_label_haha = i.attrs\n",
        "          ha = aria_label_haha['aria-label']\n",
        "        if ha == '':\n",
        "          react_haha.append('Haha: 0 people')\n",
        "        else:\n",
        "          react_haha.append(ha)\n",
        "\n",
        "        # Wow\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Wow:')}):\n",
        "          aria_label_wow = i.attrs\n",
        "          ww = aria_label_wow['aria-label']\n",
        "        if ww == '':\n",
        "          react_wow.append('Wow: 0 people')\n",
        "        else:\n",
        "          react_wow.append(ww)\n",
        "\n",
        "        # Sad\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Sad:')}):\n",
        "          aria_label_sad = i.attrs\n",
        "          sd = aria_label_sad['aria-label']\n",
        "        if sd == '':\n",
        "          react_sad.append('Sad: 0 people')\n",
        "        else:\n",
        "          react_sad.append(sd)\n",
        "\n",
        "        # Angry\n",
        "        for i in el.find_all(\"div\", {'aria-label': re.compile(r'^Angry:')}):\n",
        "          aria_label_angry = i.attrs\n",
        "          ag = aria_label_angry['aria-label']\n",
        "        if ag == '':\n",
        "          react_angry.append('Angry: 0 people')\n",
        "        else:\n",
        "          react_angry.append(ag)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        pub_attr = links[1].attrs\n",
        "        publisher = pub_attr['aria-label']\n",
        "\n",
        "        date_attr = links[3].attrs\n",
        "        date = date_attr['aria-label']\n",
        "\n",
        "\n",
        "        likes_attr = attr[4].attrs\n",
        "        likes = likes_attr['aria-label']\n",
        "\n",
        "        company.append(publisher)\n",
        "        post_date.append(date)\n",
        "        brand_posts.append(post.text.strip())\n",
        "        post_comments.append(comment_cnt)\n",
        "        \n",
        "    except:\n",
        "      pass\n"
      ],
      "execution_count": 321,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-KG_3aEl6d8"
      },
      "source": [
        "#Stripping non-numeric values\n",
        "like_count = []\n",
        "love_count = []\n",
        "care_count = []\n",
        "haha_count = []\n",
        "wow_count = []\n",
        "sad_count = []\n",
        "angry_count = []\n",
        "shares_count = []\n",
        "\n",
        "for i in react_like:\n",
        "    s = str(i).replace('Like:','').replace('people','').replace('person','').replace(' ','')\n",
        "    like_count += [s]\n",
        "    for i in range(0, len(like_count)):\n",
        "        like_count[i] = int(like_count[i])\n",
        "\n",
        "for i in react_love:\n",
        "    s = str(i).replace('Love:','').replace('people','').replace('person','').replace(' ','')\n",
        "    love_count += [s]\n",
        "    for i in range(0, len(love_count)):\n",
        "        love_count[i] = int(love_count[i])\n",
        "\n",
        "for i in react_care:\n",
        "    s = str(i).replace('Care:','').replace('people','').replace('person','').replace(' ','')\n",
        "    care_count += [s]\n",
        "    for i in range(0, len(care_count)):\n",
        "        care_count[i] = int(care_count[i])\n",
        "\n",
        "for i in react_haha:\n",
        "    s = str(i).replace('Haha:','').replace('people','').replace('person','').replace(' ','')\n",
        "    haha_count += [s]\n",
        "    for i in range(0, len(haha_count)):\n",
        "        haha_count[i] = int(haha_count[i])\n",
        "\n",
        "for i in react_wow:\n",
        "    s = str(i).replace('Wow:','').replace('people','').replace('person','').replace(' ','')\n",
        "    wow_count += [s]\n",
        "    for i in range(0, len(wow_count)):\n",
        "        wow_count[i] = int(wow_count[i])\n",
        "\n",
        "for i in react_sad:\n",
        "    s = str(i).replace('Sad:','').replace('people','').replace('person','').replace(' ','')\n",
        "    sad_count += [s]\n",
        "    for i in range(0, len(sad_count)):\n",
        "        sad_count[i] = int(sad_count[i])\n",
        "\n",
        "for i in react_angry:\n",
        "    s = str(i).replace('Angry:','').replace('people','').replace('person','').replace(' ','')\n",
        "    angry_count += [s]\n",
        "    for i in range(0, len(angry_count)):\n",
        "        angry_count[i] = int(angry_count[i])\n",
        "\n",
        "for i in post_shares:\n",
        "    s = str(i).replace('Shares','').replace('Share','').replace('person','').replace(' ','')\n",
        "    shares_count += [s]\n",
        "    for i in range(0, len(shares_count)):\n",
        "        shares_count[i] = int(shares_count[i])\n"
      ],
      "execution_count": 323,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZwLL1sWljo"
      },
      "source": [
        "**USER ACTION: Set the name of the .csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZTm39Xx8mWq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "02762c2f-a4b3-4f43-8e1e-0179d6f54a0f"
      },
      "source": [
        "data = {\n",
        "    \"Company\": company,\n",
        "    \"Date\": post_date,\n",
        "    \"Post\": brand_posts,\n",
        "    \"People\": post_commentors,\n",
        "    \"Like\": like_count,\n",
        "    \"Love\": love_count,\n",
        "    \"Care\": care_count,\n",
        "    \"Haha\": haha_count,\n",
        "    \"Wow\": wow_count,\n",
        "    \"Sad\": sad_count,\n",
        "    \"Angry\": angry_count,\n",
        "    \"Comments\": post_comments,\n",
        "    \"Shares\": shares_count\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n"
      ],
      "execution_count": 324,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-324-4b83dac7bebf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m }\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         ]\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype, verify_integrity)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    395\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 397\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"arrays must all be same length\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: arrays must all be same length"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpXrOWpsjonW"
      },
      "source": [
        "reactions = ['Like','Love','Care','Haha','Wow','Sad','Angry','Comments','Shares']\n",
        "df['Engagement'] = df[reactions].sum(axis=1)"
      ],
      "execution_count": 315,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdx5ITepjkY0"
      },
      "source": [
        "#Exporting as csv file to program folder\n",
        "df.to_csv(\"ch.csv\", encoding='utf-8', index=False)"
      ],
      "execution_count": 316,
      "outputs": []
    }
  ]
}